{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_sentiment_analysis_with_nn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfAI/tutorials/blob/master/Reti%20Neurali%20per%20la%20Sentiment%20Analysis%20con%20Keras/imdb_sentiment_analysis_with_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K02b3oX8Z9kr",
        "colab_type": "text"
      },
      "source": [
        "## Come creare una Rete Neurale per la Sentiment Analysis con Keras\n",
        "La Sentiment Analysis è uno dei campi più popolari del Natural Language Processing, il suo scopo è quello di classificare documenti di testo, come post, commenti o recensioni, in base alla loro poralità, cioè in base al sentimento positivo o negativo espresso nel testo. Le principali tecniche per la Sentiment Analysis possono essere raggruppate in 2 categorie: lessicali e statistichel. Le ultime, grazie ai progressi esponenziali nel settore del machine learning e all'applicazione delle reti neurali artificiali, sono diventate estremamente popolari. In questo tutorial costruiremo una rete neurale artificiale per eseguire la sentiment analysis di recensioni di film usando Keras, una popolare libreria per il Deep Learning, che funziona al di sopra di altre librerie per il calcolo tensoriale come Tensorflow, CNTK o Theano, e permette di sviluppare diverse architetture di reti neurali artificiali con poche righe di codice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cLyt3c3eoovG"
      },
      "source": [
        "## Prerequisiti\n",
        "Per seguire questo tutorial ai bisogno di una certa familiarità con il Machine Learning, se non sai proprio nulla a riguardo [parti da qui](http://blog.profession.ai/cosa-e-machine-learning/) e poi dai uno sguardo [a questo](http://blog.profession.ai/deep-learning-svelato-ecco-come-funzionano-le-reti-neurali-artificiali/).\n",
        "<br>\n",
        "Al momento la versione preinstallata in Colaboratory ha un bug che la rende incompatibile con alcune funzioni di Keras, quindi eseguiamo il downgrade alla versione precedente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXPhXVlTdwpl",
        "colab_type": "code",
        "outputId": "3fede64c-7b10-4ae9-97d1-5aa84ec26741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install numpy==1.16.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXNl3qWluZ3p",
        "colab_type": "text"
      },
      "source": [
        "## Step 1 - Procuriamoci il Dataset\n",
        "Il dataset che utilizzeremo per addestrare la nostra rete neurale è l'IMDB Movie Review Dataset, che contiene 50.000  esempi di recensioni di film (25.000 per l'addestramento e 25.000 per il testing) correttamente etichettate come positive o negative. Possiamo importare il dataset dentro un'array numpy usando le API di Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO2Rd-f8ZsKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb \n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7o2XOjSvMn5",
        "colab_type": "text"
      },
      "source": [
        "Il parametro *num_words* ci serve per definire il numero massimo di parole più frequenti da considerare. Così facendo abbiamo ottenuto 4 array:\n",
        "* *X_train* che contiene le features degli esempi per l'addestramento.\n",
        "* *y_train* che contiene i target per l'addestramento, cioè un singolo valore 0 o 1 che indicano rispettivamente se la recensione è negativa o positiva.\n",
        "* *X_test* che contiene le features degli esempi per l'addestramento.\n",
        "* *y_test* che contiene i target il test.\n",
        "\n",
        "Ogni riga degli array con le features corrisponde ad una recensione, le recensioni sono già state codificate in numeri, ognuna di esse è infatti una lista di numeri dove ogni numero corrisponde alla posizione della corrispondente parola all'interno del vocabolario dell'intero corpus di testo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG-kA7CXvzLW",
        "colab_type": "code",
        "outputId": "0dea3f0e-8a50-43ba-bda5-a3200c151163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train[159]) # [1, 6, 675, 7, 300, 127, 24, 895, 8, 2623, 89, 753, 2279, 5, 2, 78, 14, 20, 9]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 6, 675, 7, 300, 127, 24, 895, 8, 2623, 89, 753, 2279, 5, 2, 78, 14, 20, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ONUFgzw8LU",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 (opzionale) - Come decodificare una recensione"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIzEVY1XwMOg",
        "colab_type": "text"
      },
      "source": [
        "Nonostante non sia essenziale ai fini della costruzione del modello, può essere utile conoscere come decodificare una recensione per risalire al testo originale. Per farlo dobbiamo procurarci il vocabolario che mappa le parole agli indici ed utilizzarlo per costruire un dizionario con la relazione inversa, cioè che mappa gli indici alle parole."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPhYejRneJWv",
        "colab_type": "code",
        "outputId": "0046e275-6b70-405d-a41f-d93d442c73e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-9EPunqxLcU",
        "colab_type": "text"
      },
      "source": [
        "Ora possiamo usare il dizionario *reverse_word_index* per decodificare una recensione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqeYM1NVeFoC",
        "colab_type": "code",
        "outputId": "5b5b4035-0170-490c-cc9b-18e1d32bed77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoded_review = [word_index.get(i - 3, '?') for i in X_train[159]]\n",
        "decoded_review = ' '.join(decoded_review)\n",
        "print(decoded_review) # ? a rating of 1 does not begin to express how dull depressing and ? bad this movie is"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "? a rating of 1 does not begin to express how dull depressing and ? bad this movie is\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-SODbnisdmy",
        "colab_type": "text"
      },
      "source": [
        "Gli indici delle parole hanno un offset di 3 rispetto al vocabolario, per questo otteniamo la parola corrispondente facendo *i-3*,  utilizzando il metodo *join* delle stringhe uniamo la lista di parole ottenuta dalla list comprehensions dividendole con degli spazi, in modo da ottenere una stringa. Dato che abbiamo limitato il vocabolario a solamente le 5000 parole più frequenti, alcune parole presenti all'interno di una recensione potrebbero essere mancanti, al loro posto verrà inserito un punto interrogativo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rmttI_myJyl",
        "colab_type": "text"
      },
      "source": [
        "## Step 3 - Preprocessiamo i dati\n",
        "Utilizziamo il One Hot Encoding per creare quelle che saranno le features del nostro modello. Il one hot encoding si esegue creando, per ogni recensione, un array di lunghezza pari al numero totale di parole presenti all'interno dell'intero corpus di testo e inserendo dei valori 1 agli indici corrispondenti alle parole presenti nella frase e dei valori 0 altrimenti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nfhNG1pgI82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def onehot_encoding(data, size):\n",
        "    onehot = np.zeros((len(data), size))\n",
        "    for i, d in enumerate(data):\n",
        "        onehot[i,d] = 1.\n",
        "    return onehot\n",
        "  \n",
        "X_train = onehot_encoding(X_train, 5000) # len = (25000, 5000)\n",
        "X_test = onehot_encoding(X_test, 5000) # len = (25000, 5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUop6iGmuYnK",
        "colab_type": "text"
      },
      "source": [
        "Il risultato di questa operazione è una matrice sparsa, cioè una matrice contenete per la maggior parte dei valori 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78dPf9EzWzyK",
        "colab_type": "text"
      },
      "source": [
        "## Step 4 - Costruiamo la Rete Neurale\n",
        "Una rete neurale artificiale è un modello di machine learning che riesce apprendere relazioni non lineari nei dati, anche molto complesse, ispirandosi al funzionamento del cervello animale. Diversi neuroni sono disposti su diversi strati in sequenza e i neuroni di strati successivi sono connessi ai neuroni degli strati precedenti tramite dei pesi. Il primo strato di una rete neurale prende in input le features, l'ultimo strato fornisce l'output della rete, mentre gli strati intermedi, chiamati anche strati nascosti, utilizzano le features provenienti dallo strato precedente per apprendere nuove features più significative per l'obiettivo della nostra rete. Nell'ambito del deep learning i neuroni vengono chiamati anche unità o nodi, noi adotteremo quest'ultimo termine.\n",
        "\n",
        "La rete neurale che creeremo avrà la seguente architettura:\n",
        "\n",
        "* Uno strato di input con 512 nodi, cioè il numero di features del dataset.\n",
        "* Uno strato nascosto con 128 nodi.\n",
        "* Uno strato nascosto con 32 nodi.\n",
        "* Uno strato nascosto con 8 nodi.\n",
        "* Un'ultimo strato di output, con un solo nodo che conterrà l'output della rete, cioè il risultato della classificazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkioMZFngLwr",
        "colab_type": "code",
        "outputId": "61e510b9-3a1a-4034-ef2d-985af2035afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(5000,)))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0713 15:10:18.701968 139770219632512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0713 15:10:18.746067 139770219632512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0713 15:10:18.752196 139770219632512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKvHvh1XYP8g",
        "colab_type": "text"
      },
      "source": [
        "La classe Sequential ci permette di inizializzare un nuovo stack lineare di strati, utilizzando il metodo .add possiamo aggiungere nuovi strati. La classe Dense ci permette di creare un nuovo strato denso, cioè uno strato in cui tutti i nodi dello strato precedente sono connessi a tutti i nodi dello strato successivo tramite dei pesi. Per questa classe dobbiamo specificare dei parametri:\n",
        "* il primo parametro è il numero di nodi dello strato, per l'ultimo strato è pari al numero di output, nel nostro caso, trattandosi di un problema di classificazione binaria (recensione positiva/negativa) avremo un unico nodo di output che conterrà la probabilità che la recensione sia positiva o negativa.\n",
        "* il parametro *activation* è la funzione di attivazione da utilizzare per lo strato, quella che ci permette di ottenere risultati non lineari, la funzione di attivazione più utilizzata per gli strati nascosti è la Rectified Linear Unit (RELU), mentre per lo strato di output, trattandosi di un problema di classificazione binaria, dobbiamo utilizzare la Sigmoide.\n",
        "* il parametro *input_shape* contiene la dimensione dell'input, va specificato solo per lo strato di input, mentre per gli altri strati viene calcolato in automatico da Keras.\n",
        "\n",
        "Dopo aver definito l'architettura della rete dobbiamo configurare la fase di addestramento utilizzando il metodo *compile*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKxNISstgSGN",
        "colab_type": "code",
        "outputId": "7b718cf7-e662-47ba-eeaa-64b43861965f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0713 15:10:21.907979 139770219632512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0713 15:10:21.937448 139770219632512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0713 15:10:21.943973 139770219632512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9gBGH7N0ZwB",
        "colab_type": "text"
      },
      "source": [
        "* All'interno del parametro *loss* specifichiamo la funzione di costo da utilizzare, cioè la funzione che ci permette di calcolare la performance del nostro modello in base alla qualità delle sue predizioni. Nel caso di classificazioni binarie la funzione di costo da utilizzare è la *binary crossentropy*, che tiene conto della probabilità che il nostro modello abbiamo fornito il risultato corretto.\n",
        "* All'interno del parametro *optimizer* specifichiamo l'algoritmo di ottimizzazione da utilizzare per l'addestramento del modello. Un'algoritmo di ottimizzazione ci permette di trovare i pesi del modello che minimizzano la funzione di costo da noi specificata, l'algoritmo di ottimizzazione principale è il *Gradient Descent*, che utilizza un processo iterativo in cui, ad ogni iterazione, ogni peso viene sommata alla relativa derivata rispetto alla funzione di costo moltiplicata per un'ulteriore iperparametro chiamato *learning rate*, utilizzato per controlare la dimensione di ogni step dell' ottimizzazione. Nel nostro caso utilizzeremo *adamax*, una variante del gradient descent che dovrebbe permettere di ottenere risultati migliori nel caso in cui le features siano rappresentate da una matrice sparsa.\n",
        "* All'interno del parametro *metrics* specifichiamo una lista con altre metriche aggiuntive che ci permetteranno di misurare la qualità del modello, come l'*accuracy* che non è altro che la percentuale di classificazioni eseguite correttamente dal modello.\n",
        "\n",
        "Adesso siamo pronti per avviare la fase di addestramento utilizzando il metodo fit, alla quale dobbiamo passare gli array con i dati per l'addestramento, features e target, il numero di epoche, ovvero di iterazioni dell'algoritmo di ottimizzazione, è la dimensione di ogni batch di addestramento, cioè il numero di esempi che verranno utilizzati per uno step dell'algoritmo di ottimizzazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5qjvBabgcii",
        "colab_type": "code",
        "outputId": "92ce6d4a-2ad1-41df-eb42-978159360789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=512)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0713 15:10:24.531879 139770219632512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 5s 197us/step - loss: 0.3683 - acc: 0.8414\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 1s 42us/step - loss: 0.1997 - acc: 0.9248\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.1289 - acc: 0.9556\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.0594 - acc: 0.9854\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 1s 39us/step - loss: 0.0209 - acc: 0.9963\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 1s 38us/step - loss: 0.0069 - acc: 0.9989\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 1s 38us/step - loss: 0.0024 - acc: 0.9998\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 1s 38us/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 1s 38us/step - loss: 5.8919e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 1s 38us/step - loss: 4.0894e-04 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e631ab7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlztIZwv2_ix",
        "colab_type": "text"
      },
      "source": [
        "Alla decima epoca abbiamo ottenuto un'accuracy del 100% ed un valore per la funzione di costo tendente allo zero, questo vuol dire che il nostro modello ha classificato correttamente tutti gli esempi del set di addestramento con un grado di incertezza bassissimo. Vediamo come se la cava con recensioni che non ha già visto durante l'addestramento usando il metodo *.evaluate* sul set di test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjIioN8Eg-JE",
        "colab_type": "code",
        "outputId": "ebf0ee0b-42d2-46fe-b56a-fa9b5ed64e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 2s 62us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7001907266867161, 0.87088]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vNhsu0yAR73",
        "colab_type": "text"
      },
      "source": [
        "L'accuracy sul set di test è decisamente più bassa, mentre il valore della binary cross entropy è disastroso, come mai ? Perché il modello che abbiamo costruito ha memorizzati i dati di addestamento piuttosto che apprendere da essi e quindi ora fallisce nel generalizzare su nuovi dati. Questa condizione è conosciuta come overfitting ed è uno dei problemi principali del Machine Learning. Vediamo come possiamo risolverlo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_PWpLSwAvML",
        "colab_type": "text"
      },
      "source": [
        "## Step 5 - Contrastiamo l'overfitting\n",
        "Le soluzioni migliori per contrastare l'overfitting consistono nel ridurre la complessità del modello oppure nel raccogliere un numero maggiore di esempi per l'addestramento. Quando non è possibile far ciò possiamo adoperare delle tecniche di regolarizzazione, come:\n",
        "* le regolarizzazioni L1 ed L2: che ci permettono di penalizzare i pesi eccessivamente grandi, che sono proprio quelli che causano l'overfitting.\n",
        "* il dropout: che ci permette di \"spegnere\" una percentuale prefissata di nodi selezionati a caso, in questo modo i nodi eviteranno di farsi carico degli errori di altri nodi riducendo il rischio di overfitting.\n",
        "\n",
        "Ridefiniamo l'architettura della rete aggiungendo la regolarizzazione L2 e il dropout tra gli strati."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxm5Eg4-gkFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.regularizers import l2\n",
        "from keras.layers import Dropout\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, activation='relu', input_shape=(5000,), kernel_regularizer=l2(0.1)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128,activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(32,activation='relu',kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(8,activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi22ruxiCaFc",
        "colab_type": "text"
      },
      "source": [
        "La classe *Dropout* ha bisogno di un unico parametro, che rappresenta la percentuale di nodi da disattivare ad ogni iterazione. Anche la funzione *l2* necessita di un'unico parametro, che rappresenta il parametro di regolarizzazione, un valore che indica l'intensità della regolarizzazione da applicare. Fatto questo, riconfiguriamo la fase di addestramento e avviamola nuovamente, questa volta per 100 epoche."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRBSG2QqkgRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1e95ccc-8be2-42e8-9662-c57b41b6ac3c"
      },
      "source": [
        "model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=512)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "25000/25000 [==============================] - 2s 72us/step - loss: 12.3156 - acc: 0.5902\n",
            "Epoch 2/100\n",
            "25000/25000 [==============================] - 1s 42us/step - loss: 0.8628 - acc: 0.7631\n",
            "Epoch 3/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.7241 - acc: 0.8240\n",
            "Epoch 4/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.6826 - acc: 0.8359\n",
            "Epoch 5/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.6451 - acc: 0.8438\n",
            "Epoch 6/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.6166 - acc: 0.8529\n",
            "Epoch 7/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.6155 - acc: 0.8544\n",
            "Epoch 8/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.5849 - acc: 0.8578\n",
            "Epoch 9/100\n",
            "25000/25000 [==============================] - 1s 39us/step - loss: 0.5802 - acc: 0.8566\n",
            "Epoch 10/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.5480 - acc: 0.8672\n",
            "Epoch 11/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.5457 - acc: 0.8657\n",
            "Epoch 12/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.5409 - acc: 0.8675\n",
            "Epoch 13/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.5386 - acc: 0.8657\n",
            "Epoch 14/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.5242 - acc: 0.8697\n",
            "Epoch 15/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.5200 - acc: 0.8705\n",
            "Epoch 16/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.5088 - acc: 0.8784\n",
            "Epoch 17/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.5149 - acc: 0.8753\n",
            "Epoch 18/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.5191 - acc: 0.8705\n",
            "Epoch 19/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.5010 - acc: 0.8750\n",
            "Epoch 20/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4984 - acc: 0.8762\n",
            "Epoch 21/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4990 - acc: 0.8752\n",
            "Epoch 22/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.5026 - acc: 0.8766\n",
            "Epoch 23/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4940 - acc: 0.8775\n",
            "Epoch 24/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4928 - acc: 0.8776\n",
            "Epoch 25/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4944 - acc: 0.8758\n",
            "Epoch 26/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4876 - acc: 0.8812\n",
            "Epoch 27/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4832 - acc: 0.8835\n",
            "Epoch 28/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4818 - acc: 0.8824\n",
            "Epoch 29/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4881 - acc: 0.8821\n",
            "Epoch 30/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4910 - acc: 0.8808\n",
            "Epoch 31/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4829 - acc: 0.8824\n",
            "Epoch 32/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4783 - acc: 0.8835\n",
            "Epoch 33/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4766 - acc: 0.8850\n",
            "Epoch 34/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4800 - acc: 0.8829\n",
            "Epoch 35/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4768 - acc: 0.8894\n",
            "Epoch 36/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4763 - acc: 0.8831\n",
            "Epoch 37/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4786 - acc: 0.8830\n",
            "Epoch 38/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4767 - acc: 0.8843\n",
            "Epoch 39/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4748 - acc: 0.8849\n",
            "Epoch 40/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4792 - acc: 0.8854\n",
            "Epoch 41/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4789 - acc: 0.8822\n",
            "Epoch 42/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4705 - acc: 0.8913\n",
            "Epoch 43/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4615 - acc: 0.8923\n",
            "Epoch 44/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4672 - acc: 0.8904\n",
            "Epoch 45/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4705 - acc: 0.8903\n",
            "Epoch 46/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4660 - acc: 0.8909\n",
            "Epoch 47/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4690 - acc: 0.8916\n",
            "Epoch 48/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4624 - acc: 0.8908\n",
            "Epoch 49/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4668 - acc: 0.8892\n",
            "Epoch 50/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4645 - acc: 0.8925\n",
            "Epoch 51/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4644 - acc: 0.8910\n",
            "Epoch 52/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4657 - acc: 0.8922\n",
            "Epoch 53/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4726 - acc: 0.8892\n",
            "Epoch 54/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4619 - acc: 0.8925\n",
            "Epoch 55/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4546 - acc: 0.8968\n",
            "Epoch 56/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4558 - acc: 0.8957\n",
            "Epoch 57/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4631 - acc: 0.8938\n",
            "Epoch 58/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4643 - acc: 0.8931\n",
            "Epoch 59/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4563 - acc: 0.8950\n",
            "Epoch 60/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4749 - acc: 0.8891\n",
            "Epoch 61/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4575 - acc: 0.8956\n",
            "Epoch 62/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4545 - acc: 0.8963\n",
            "Epoch 63/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4540 - acc: 0.8974\n",
            "Epoch 64/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4470 - acc: 0.8982\n",
            "Epoch 65/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4509 - acc: 0.9005\n",
            "Epoch 66/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4508 - acc: 0.8989\n",
            "Epoch 67/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4549 - acc: 0.8987\n",
            "Epoch 68/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4524 - acc: 0.9005\n",
            "Epoch 69/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4492 - acc: 0.9003\n",
            "Epoch 70/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4457 - acc: 0.9020\n",
            "Epoch 71/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4480 - acc: 0.9012\n",
            "Epoch 72/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4455 - acc: 0.9022\n",
            "Epoch 73/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4490 - acc: 0.9022\n",
            "Epoch 74/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4518 - acc: 0.8987\n",
            "Epoch 75/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4579 - acc: 0.8981\n",
            "Epoch 76/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4520 - acc: 0.8984\n",
            "Epoch 77/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4401 - acc: 0.9053\n",
            "Epoch 78/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4482 - acc: 0.9006\n",
            "Epoch 79/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4601 - acc: 0.8961\n",
            "Epoch 80/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4490 - acc: 0.9009\n",
            "Epoch 81/100\n",
            "25000/25000 [==============================] - 1s 39us/step - loss: 0.4469 - acc: 0.8995\n",
            "Epoch 82/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4475 - acc: 0.9016\n",
            "Epoch 83/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4414 - acc: 0.9057\n",
            "Epoch 84/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4457 - acc: 0.9030\n",
            "Epoch 85/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4483 - acc: 0.8991\n",
            "Epoch 86/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4480 - acc: 0.9008\n",
            "Epoch 87/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4594 - acc: 0.8985\n",
            "Epoch 88/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4408 - acc: 0.9052\n",
            "Epoch 89/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4435 - acc: 0.9032\n",
            "Epoch 90/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4485 - acc: 0.9016\n",
            "Epoch 91/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4467 - acc: 0.9040\n",
            "Epoch 92/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4481 - acc: 0.9038\n",
            "Epoch 93/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4370 - acc: 0.9072\n",
            "Epoch 94/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4379 - acc: 0.9084\n",
            "Epoch 95/100\n",
            "25000/25000 [==============================] - 1s 41us/step - loss: 0.4392 - acc: 0.9061\n",
            "Epoch 96/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4369 - acc: 0.9069\n",
            "Epoch 97/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4355 - acc: 0.9087\n",
            "Epoch 98/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4521 - acc: 0.9003\n",
            "Epoch 99/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4424 - acc: 0.9055\n",
            "Epoch 100/100\n",
            "25000/25000 [==============================] - 1s 40us/step - loss: 0.4372 - acc: 0.9085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e6ca96fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhY-WslFDUUi",
        "colab_type": "text"
      },
      "source": [
        "I valori della funzione di costo e dell'accuracy sono più realistici rispetto a prima, verifichiamo se abbiamo risolto il nostro problema di overfitting testando il modello sul set di test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7iKEItLgte0",
        "colab_type": "code",
        "outputId": "0e1f50dd-9ac2-433e-f6d4-81f27aeb1b87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        " model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 3s 100us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4818225428390503, 0.87684]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mabAKfxTDvMF",
        "colab_type": "text"
      },
      "source": [
        "Le metriche sul set di test sono migliori rispetto a prima e più vicine a quelle ottenute sul set di addestramento, specialmente la binary cross entropy, questo sta ad indicare che il modello che abbiamo addestrato è più sicuro sulle sue predizioni."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pooITLlSEAkD",
        "colab_type": "text"
      },
      "source": [
        "## Step 6 - Mettiamo la Rete Neurale all'opera\n",
        "Ora che la nostra rete neurale funziona abbastanza bene, mettiamola alla prova su nuove recensioni, cominciamo definendo una funzione che prende in input una recensione e la converte in un array numpy codificato tramite one hot encoding, pronto per essere dato in pasto alla nostra rete neurale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Xkpz58EAG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from re import sub\n",
        "\n",
        "def preprocess(review):\n",
        "  \n",
        "    # otteniamo il vocabolario\n",
        "    word_index = imdb.get_word_index()\n",
        "    \n",
        "    # Rimuoviamo un'eventuale punteggiatura utilizzando un'espressione regolare\n",
        "    review = sub(r'[^\\w\\s]','',review) \n",
        "    # Convertiamo tutto in minuscolo\n",
        "    review = review.lower()\n",
        "    # Creiamo un array di parole\n",
        "    review = review.split(\" \")\n",
        "\n",
        "    # Qui dentro metteremo gli IDs\n",
        "    review_array = []\n",
        "\n",
        "    # Iteriamo lungo le parole della recensione\n",
        "    for word in review:\n",
        "        # proseguiamo se la parola si trova all'interno\n",
        "        # della lista di parole del corpus di addestramento\n",
        "        if word in word_index:\n",
        "            # estraiamo l'indice della parola \n",
        "            index = word_index[word] \n",
        "            # Proseguiamo se l'indice è minore o uguale a 5000\n",
        "            # cioè il numero di parole che abbiamo utilizzato\n",
        "            # per l'addestramento\n",
        "            if index <= 5000:\n",
        "                # aggiungiamo l'indice all'array\n",
        "                # ricordandoci dell'offset di 3\n",
        "                review_array.append(word_index[word]+3)\n",
        "                \n",
        "    # Eseguiamo il one hot encoding sulla lista di indici\n",
        "    print(review_array)\n",
        "    review_array = onehot_encoding([review_array], 5000)\n",
        "    return review_array\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM_iXbQcEuo8",
        "colab_type": "text"
      },
      "source": [
        "L'ouptut della nostra rete neurale sarà un valore compreso tra 0 ed 1 che indica la probabilità che la recensione sia positiva, quindi un output di 0 indicherà una recensione sicuramente non positiva (e quindi negativa), mentre un valore di 1 indicherà una recensione sicuramente positiva. Definiamo una funzione che prendendo in input questo valore ritorna una stringa che ne rappresenta il sentiment associato"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plt9VuPVFWUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prob_to_sentiment(prob):\n",
        "    \n",
        "    if(prob>0.9): return \"fantastica\"\n",
        "    elif(prob>0.75): return \"ottima\"\n",
        "    elif(prob>0.55): return \"buona\" \n",
        "    elif(prob>0.45): return \"neutrale\"\n",
        "    elif(prob>0.25): return \"negativa\"\n",
        "    elif(prob>0.1): return \"brutta\"\n",
        "    else: return \"pessima\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHmGTGmxFZQW",
        "colab_type": "text"
      },
      "source": [
        "Ora mettiamo tutto insieme per classifcare qualche recensione pescata dal web, cominciamo con una relativa a uno dei film più brutti che ho avuto la sciagura di vedere: Paranormal Activity 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thb6fFfKFX-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "e9d0dff5-7eda-443d-b79a-acb262194374"
      },
      "source": [
        "review = \"what a waste of time and cash.. the movie was pointless. with no flow. no questions answered. just a waste. I never review movies but had to share how bad this was..compared to part 1- 2- and 3.... i don't know what else to say other than how misleading the commercial is.. the commercial was cut and spliced with video and audio that didn't even match what happened in the movie... you have been warned. when the movie was over.. people actually Boo'd. hopefully people will spread the word, and save others from throwing their money away. i know die-hard fans will go and give it a shot, but will be disappointed as well. Sinister was better and actually made you jump quite a few times.\"\n",
        "x = preprocess(review)\n",
        "y = model.predict(x)[0]\n",
        "print(\"REVIEW:\", review)\n",
        "print(\"\\n\")\n",
        "print(\"La recensione è %s [%.6f]\" % (prob_to_sentiment(y), y))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[51, 6, 437, 7, 58, 5, 2208, 4, 20, 16, 1149, 19, 57, 2973, 57, 1204, 43, 6, 437, 13, 115, 733, 102, 21, 69, 8, 1497, 89, 78, 14, 8, 173, 300, 241, 5, 342, 13, 124, 51, 334, 8, 135, 85, 74, 89, 4, 2153, 9, 4, 2153, 16, 605, 5, 19, 374, 5, 3884, 15, 60, 1014, 51, 575, 11, 4, 20, 25, 28, 77, 2815, 54, 4, 20, 16, 120, 84, 165, 2363, 84, 80, 4600, 4, 681, 5, 607, 409, 39, 2825, 68, 278, 245, 13, 124, 451, 80, 140, 5, 202, 12, 6, 324, 21, 80, 30, 685, 17, 73, 2950, 16, 128, 5, 165, 93, 25, 1783, 179, 6, 171, 211]\n",
            "REVIEW: what a waste of time and cash.. the movie was pointless. with no flow. no questions answered. just a waste. I never review movies but had to share how bad this was..compared to part 1- 2- and 3.... i don't know what else to say other than how misleading the commercial is.. the commercial was cut and spliced with video and audio that didn't even match what happened in the movie... you have been warned. when the movie was over.. people actually Boo'd. hopefully people will spread the word, and save others from throwing their money away. i know die-hard fans will go and give it a shot, but will be disappointed as well. Sinister was better and actually made you jump quite a few times.\n",
            "\n",
            "\n",
            "La recesione è pessima [0.051200]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wgBICupFmPz",
        "colab_type": "text"
      },
      "source": [
        "La nostra rete indica che la recensione è (ovviamente) pessima, ma proprio tanto tanto. Proviamo adesso con una recensione che riguarda Avengers: Infinity War.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm6Ph2TzFnP8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "8f5f6432-9dd4-48b5-fe75-23a60042427b"
      },
      "source": [
        "review = \"This movie will blow your mind and break your heart - and make you desperate to go back for more. Brave, brilliant and better than it has any right to be.\"\n",
        "x = preprocess(review)\n",
        "y = model.predict(x)\n",
        "\n",
        "print(\"REVIEW:\", review)\n",
        "print(\"\\n\")\n",
        "print(\"La recensione è %s [%.6f]\" % (prob_to_sentiment(y), y))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14, 20, 80, 2479, 129, 330, 5, 989, 129, 483, 5, 97, 25, 1680, 8, 140, 145, 18, 53, 2510, 530, 5, 128, 74, 12, 47, 101, 208, 8, 30]\n",
            "REVIEW: This movie will blow your mind and break your heart - and make you desperate to go back for more. Brave, brilliant and better than it has any right to be.\n",
            "\n",
            "\n",
            "La recesione è fantastica [0.915910]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6LxPRhFFrCW",
        "colab_type": "text"
      },
      "source": [
        "La rete dice che la recensione è positiva (dai, a chi non è piaciuto questo film ?). Se vuoi divertirti un po' prova a scrivere la tua recensione, tenendo conto che, dato che abbiamo addestrato la rete su di un corpus di testo inglese, la recensione deve essere in lingua inglese."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqxESQvWFtN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review = input(\"Write your review: \")\n",
        "x = preprocess(review)\n",
        "prob = model.predict(review)\n",
        "\n",
        "print(\"REVIEW: %s\" % review)\n",
        "print(\"\\n\")\n",
        "print(\"La recensione è %s [%.6f]\" % (prob_to_sentiment(prob), prob))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}